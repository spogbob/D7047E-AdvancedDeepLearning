{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Tensorboard for visualizing\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Tiny Shakespeare"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model from https://github.com/spro/char-rnn.pytorch/blob/master/model.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward2(self, input, hidden):\n",
    "        encoded = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### helpers.py from https://github.com/spro/char-rnn.pytorch/blob/master/helpers.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "# Reading and un-unicode-encoding data\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "# Turning a string into a tensor\n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor\n",
    "\n",
    "# Readable time elapsed\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate.py from https://github.com/spro/char-rnn.pytorch/blob/master/generate.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hear me here all him.\n",
      "\n",
      "CORIO:\n",
      "How fares been, sir, that he at this pition,\n",
      "No fortune, as the times\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=True):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
    "\n",
    "    if cuda:\n",
    "        hidden = hidden.cuda()\n",
    "        prime_input = prime_input.cuda()\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "\n",
    "    inp = prime_input[:,-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "\n",
    "    return predicted\n",
    "\n",
    "# Run as standalone script\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "# Parse command line arguments\n",
    "#     argparser = argparse.ArgumentParser()\n",
    "#     argparser.add_argument('filename', type=str)\n",
    "#     argparser.add_argument('-p', '--prime_str', type=str, default='A')\n",
    "#     argparser.add_argument('-l', '--predict_len', type=int, default=100)\n",
    "#     argparser.add_argument('-t', '--temperature', type=float, default=0.8)\n",
    "#     argparser.add_argument('--cuda', action='store_true')\n",
    "#     args = argparser.parse_args()\n",
    "\n",
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"The\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train.py from https://github.com/spro/char-rnn.pytorch/blob/master/train.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training for 2000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 606/2000 [03:58<09:27,  2.46it/s]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Parse command line arguments\n",
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument('filename', type=str, default='tinyShakespear.txt')\n",
    "# argparser.add_argument('--model', type=str, default=\"gru\")\n",
    "# argparser.add_argument('--n_epochs', type=int, default=2000)\n",
    "# argparser.add_argument('--print_every', type=int, default=100)\n",
    "# argparser.add_argument('--hidden_size', type=int, default=100)\n",
    "# argparser.add_argument('--n_layers', type=int, default=2)\n",
    "# argparser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "# argparser.add_argument('--chunk_len', type=int, default=200)\n",
    "# argparser.add_argument('--batch_size', type=int, default=100)\n",
    "# argparser.add_argument('--shuffle', action='store_true')\n",
    "# argparser.add_argument('--cuda', action='store_true')\n",
    "# args = argparser.parse_args()\n",
    "\n",
    "arg_filename = \"tinyShakespear.txt\"\n",
    "arg_model = \"gru\"\n",
    "arg_n_epochs = 2000\n",
    "arg_print_every = 2000\n",
    "arg_hidden_size = 100\n",
    "arg_n_layers = 2\n",
    "arg_learning_rate = 0.01\n",
    "arg_chunk_len = 200\n",
    "arg_batch_size = 100\n",
    "arg_shuffle = True\n",
    "arg_cuda = True\n",
    "\n",
    "\n",
    "if arg_cuda:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "file, file_len = read_file(arg_filename)\n",
    "\n",
    "def make_vocab(input_seq, input_len):\n",
    "    my_vocab = []\n",
    "    my_word = \"\"\n",
    "    for i in range(input_len):\n",
    "        my_char = input_seq[i]\n",
    "        if my_char == ' ' or my_char == '\\n' or my_char == '\\0':\n",
    "            if my_word.len() > 0:\n",
    "                my_vocab.append(my_word)\n",
    "            my_word = \"\"\n",
    "        else:\n",
    "            my_word = my_word + my_char;\n",
    "    return my_vocab\n",
    "\n",
    "vocab = make_vocab(read_file(arg_filename))\n",
    "\n",
    "def random_training_set(chunk_len, batch_size):\n",
    "    inp = torch.LongTensor(batch_size, chunk_len)\n",
    "    target = torch.LongTensor(batch_size, chunk_len)\n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, file_len - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        set_vocab = make_vocab(chunk, chunk.len())\n",
    "        #for each element in set_vocab, match to index of same word in vocab\n",
    "        inp[bi] = char_tensor(chunk[:-1])\n",
    "        target[bi] = char_tensor(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    if arg_cuda:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    return inp, target\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden(arg_batch_size)\n",
    "    if arg_cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(arg_chunk_len):\n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss += criterion(output.view(arg_batch_size, -1), target[:,c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/ arg_chunk_len\n",
    "\n",
    "def save():\n",
    "    save_filename = os.path.splitext(os.path.basename(arg_filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "# Initialize models and start training\n",
    "\n",
    "decoder = CharRNN(\n",
    "    n_characters,\n",
    "    arg_hidden_size,\n",
    "    n_characters,\n",
    "    model = arg_model,\n",
    "    n_layers = arg_n_layers\n",
    ")\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=arg_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if arg_cuda:\n",
    "    decoder.cuda()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "try:\n",
    "    print(\"Training for %d epochs...\" % arg_n_epochs)\n",
    "    for epoch in tqdm(range(1, arg_n_epochs + 1)):\n",
    "        loss = train(*random_training_set(arg_chunk_len, arg_batch_size))\n",
    "        writer.add_scalar(\"Perplexity\", np.exp(loss), epoch)\n",
    "        loss_avg += loss\n",
    "\n",
    "        if epoch % arg_print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / arg_n_epochs * 100, loss))\n",
    "            print(generate(decoder, 'Wh', 100, cuda=arg_cuda), '\\n')\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saving before quit...\")\n",
    "    save()\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2eF 3hom at your privilued, nor\n",
      "Some company, our husband of not to pursue.\n",
      "\n",
      "RICHMOND:\n",
      "No loddle of a fire\n",
      "\n",
      "\n",
      "\n",
      "fdghject you to twell'd, sir.\n",
      "\n",
      "Nurse:\n",
      "The shoppited conference, uncle, stay, but no,\n",
      "Which art thou bed, \n",
      "\n",
      "\n",
      "\n",
      "AbCd3 GOZUMWARD:\n",
      "So shall I prepore, and alive me up.\n",
      "3 KING HENRY VI\n",
      "\n",
      "PROSPERO:\n",
      "Before; thyself at the p\n"
     ]
    }
   ],
   "source": [
    " def simple_tokenizer(text):\n",
    "    '''\n",
    "    Takes a text and returns a list of the individual words\n",
    "    Args:\n",
    "        text (str): The text to tokenize\n",
    "    Returns ([str]): A list of individual words\n",
    "    '''\n",
    "    ## WRITE A TOKENIZER (Hint: use the 'split' function)\n",
    "    return text.split()\n",
    "\n",
    "class SimpleVocabulary():\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        '''\n",
    "        Takes a corpus, tokenizes it, and sets the internal variables needed\n",
    "        to return the index of any given word (and what word has a given index)\n",
    "        Args:\n",
    "            corpus ([str]): The corpus as a list of non-tokenized texts\n",
    "        '''\n",
    "        ## WRITE A FUNCTION TO INITIALIZE THE VOCABULARY\n",
    "        # HINT: Create a dictionary that contains the index for each unique word and antother to contain the word for a given index\n",
    "        # HINT: Each new, previously unseen token from the tokenized corpus, should be entered into the dictionaries\n",
    "\n",
    "        current_index = 0\n",
    "        self.token2index = {}\n",
    "        self.index2token = []\n",
    "        for text in corpus:\n",
    "            for word in simple_tokenizer(text):\n",
    "                if word not in self.token2index:\n",
    "                    self.index2token.append(word)\n",
    "                    self.token2index.update({word : current_index})\n",
    "                    current_index += 1\n",
    "\n",
    "\n",
    "    def get_index(self, token):\n",
    "        '''\n",
    "        Takes a token and returns the index of that token\n",
    "        Args:\n",
    "            token (str): A token\n",
    "        Returns (int): The index of the given token\n",
    "        '''\n",
    "        ## WRITE A FUNCTION THAT RETURNS THE INDEX OF A GIVEN TOKEN\n",
    "        return self.token2index.get(token)\n",
    "\n",
    "\n",
    "    def get_token(self, index):\n",
    "        '''\n",
    "        Takes an index and returns the token that index represents\n",
    "        Args:\n",
    "            index (int): An index\n",
    "        Returns (str): The token that the given index represents\n",
    "        '''\n",
    "        ## WRITE A FUNCTION THAT RETURNS THE TOKEN OF A GIVEN INDEX\n",
    "        return self.index2token[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns (int): The number of words in the vocabulary\n",
    "        '''\n",
    "        ## WRITE A FUNCTION THAT RETURNS THE LENGTH OF THE VOCABULARY\n",
    "        return len(self.token2index)\n",
    "\n",
    "def index_sequence(sequence, vocabulary):\n",
    "    '''\n",
    "    Takes a tokenized text and a vocabulary and returns an indexed sequence\n",
    "    Args:\n",
    "        sequence ([str]): A list of individual tokens\n",
    "    Returns ([int]): A list of indexes\n",
    "    '''\n",
    "\n",
    "    return [vocabulary.get_index(token) for token in sequence]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart not the present Mowbray\n",
      "But the clouded the regree and men\n",
      "They shall chief thoughs so soul t\n",
      "\n",
      "\n",
      "\n",
      "What is here?\n",
      "\n",
      "CATESBY:\n",
      "Why, blush more action, loyal know my morning\n",
      "time. Let me not, holy hand in the ch\n",
      "\n",
      "\n",
      "\n",
      "Shall I give the bright his blood,\n",
      "Than be up the heavens, the counsel of the sile\n",
      "for the marters combard hath \n",
      "\n",
      "\n",
      "\n",
      "X087hNYB BHN BYFVuhsdbsoe: full\n",
      "The spirit death, for the faker!\n",
      "\n",
      "LUCIO:\n",
      "Mistressly soul will the cover for that put\n",
      "Finlma\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}