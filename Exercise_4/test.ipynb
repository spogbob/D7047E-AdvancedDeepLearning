{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Tensorboard for visualizing\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Tiny Shakespeare"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model from https://github.com/spro/char-rnn.pytorch/blob/master/model.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward2(self, input, hidden):\n",
    "        encoded = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### helpers.py from https://github.com/spro/char-rnn.pytorch/blob/master/helpers.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "# Reading and un-unicode-encoding data\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "# Turning a string into a tensor\n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor\n",
    "\n",
    "# Readable time elapsed\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate.py from https://github.com/spro/char-rnn.pytorch/blob/master/generate.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hear me here all him.\n",
      "\n",
      "CORIO:\n",
      "How fares been, sir, that he at this pition,\n",
      "No fortune, as the times\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=True):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
    "\n",
    "    if cuda:\n",
    "        hidden = hidden.cuda()\n",
    "        prime_input = prime_input.cuda()\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "\n",
    "    inp = prime_input[:,-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "\n",
    "    return predicted\n",
    "\n",
    "# Run as standalone script\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "# Parse command line arguments\n",
    "#     argparser = argparse.ArgumentParser()\n",
    "#     argparser.add_argument('filename', type=str)\n",
    "#     argparser.add_argument('-p', '--prime_str', type=str, default='A')\n",
    "#     argparser.add_argument('-l', '--predict_len', type=int, default=100)\n",
    "#     argparser.add_argument('-t', '--temperature', type=float, default=0.8)\n",
    "#     argparser.add_argument('--cuda', action='store_true')\n",
    "#     args = argparser.parse_args()\n",
    "\n",
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"The\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train.py from https://github.com/spro/char-rnn.pytorch/blob/master/train.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training for 2000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DY GREY:\n",
      "My mind will never grant what I perceive\n",
      "Your highness aims at, if I aim aright.\n",
      "\n",
      "KING EDWARD IV:\n",
      "To tell thee plain, I aim to lie with thee.\n",
      "\n",
      "LADY GREY:\n",
      "To tell you plain, I had rather lie i\n",
      "Y GREY:\n",
      "My mind will never grant what I perceive\n",
      "Your highness aims at, if I aim aright.\n",
      "\n",
      "KING EDWARD IV:\n",
      "To tell thee plain, I aim to lie with thee.\n",
      "\n",
      "LADY GREY:\n",
      "To tell you plain, I had rather lie in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'len'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 107>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining for \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m epochs...\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m arg_n_epochs)\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, arg_n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[1;32m--> 110\u001B[0m     loss \u001B[38;5;241m=\u001B[39m train(\u001B[38;5;241m*\u001B[39m\u001B[43mrandom_training_set\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_chunk_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg_batch_size\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    111\u001B[0m     writer\u001B[38;5;241m.\u001B[39madd_scalar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPerplexity\u001B[39m\u001B[38;5;124m\"\u001B[39m, np\u001B[38;5;241m.\u001B[39mexp(loss), epoch)\n\u001B[0;32m    112\u001B[0m     loss_avg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36mrandom_training_set\u001B[1;34m(chunk_len, batch_size)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(chunk[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(chunk[\u001B[38;5;241m1\u001B[39m:])\n\u001B[1;32m---> 55\u001B[0m \u001B[43mchunk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlen\u001B[49m\n\u001B[0;32m     56\u001B[0m inp[bi] \u001B[38;5;241m=\u001B[39m char_tensor(chunk[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     57\u001B[0m target[bi] \u001B[38;5;241m=\u001B[39m char_tensor(chunk[\u001B[38;5;241m1\u001B[39m:])\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'len'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Parse command line arguments\n",
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument('filename', type=str, default='tinyShakespear.txt')\n",
    "# argparser.add_argument('--model', type=str, default=\"gru\")\n",
    "# argparser.add_argument('--n_epochs', type=int, default=2000)\n",
    "# argparser.add_argument('--print_every', type=int, default=100)\n",
    "# argparser.add_argument('--hidden_size', type=int, default=100)\n",
    "# argparser.add_argument('--n_layers', type=int, default=2)\n",
    "# argparser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "# argparser.add_argument('--chunk_len', type=int, default=200)\n",
    "# argparser.add_argument('--batch_size', type=int, default=100)\n",
    "# argparser.add_argument('--shuffle', action='store_true')\n",
    "# argparser.add_argument('--cuda', action='store_true')\n",
    "# args = argparser.parse_args()\n",
    "\n",
    "arg_filename = \"tinyShakespear.txt\"\n",
    "arg_model = \"gru\"\n",
    "arg_n_epochs = 2000\n",
    "arg_print_every = 2000\n",
    "arg_hidden_size = 100\n",
    "arg_n_layers = 2\n",
    "arg_learning_rate = 0.01\n",
    "arg_chunk_len = 200\n",
    "arg_batch_size = 100\n",
    "arg_shuffle = True\n",
    "arg_cuda = True\n",
    "\n",
    "\n",
    "if arg_cuda:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "file, file_len = read_file(arg_filename)\n",
    "\n",
    "def random_training_set(chunk_len, batch_size):\n",
    "    inp = torch.LongTensor(batch_size, chunk_len)\n",
    "    target = torch.LongTensor(batch_size, chunk_len)\n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, file_len - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        inp[bi] = char_tensor(chunk[:-1])\n",
    "        target[bi] = char_tensor(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    if arg_cuda:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    return inp, target\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden(arg_batch_size)\n",
    "    if arg_cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(arg_chunk_len):\n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss += criterion(output.view(arg_batch_size, -1), target[:,c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/ arg_chunk_len\n",
    "\n",
    "def save():\n",
    "    save_filename = os.path.splitext(os.path.basename(arg_filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "# Initialize models and start training\n",
    "\n",
    "decoder = CharRNN(\n",
    "    n_characters,\n",
    "    arg_hidden_size,\n",
    "    n_characters,\n",
    "    model = arg_model,\n",
    "    n_layers = arg_n_layers\n",
    ")\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=arg_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if arg_cuda:\n",
    "    decoder.cuda()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "try:\n",
    "    print(\"Training for %d epochs...\" % arg_n_epochs)\n",
    "    for epoch in tqdm(range(1, arg_n_epochs + 1)):\n",
    "        loss = train(*random_training_set(arg_chunk_len, arg_batch_size))\n",
    "        writer.add_scalar(\"Perplexity\", np.exp(loss), epoch)\n",
    "        loss_avg += loss\n",
    "\n",
    "        if epoch % arg_print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / arg_n_epochs * 100, loss))\n",
    "            print(generate(decoder, 'Wh', 100, cuda=arg_cuda), '\\n')\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saving before quit...\")\n",
    "    save()\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [38]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mchunk\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'chunk' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2eF 3hom at your privilued, nor\n",
      "Some company, our husband of not to pursue.\n",
      "\n",
      "RICHMOND:\n",
      "No loddle of a fire\n",
      "\n",
      "\n",
      "\n",
      "fdghject you to twell'd, sir.\n",
      "\n",
      "Nurse:\n",
      "The shoppited conference, uncle, stay, but no,\n",
      "Which art thou bed, \n",
      "\n",
      "\n",
      "\n",
      "AbCd3 GOZUMWARD:\n",
      "So shall I prepore, and alive me up.\n",
      "3 KING HENRY VI\n",
      "\n",
      "PROSPERO:\n",
      "Before; thyself at the p\n"
     ]
    }
   ],
   "source": [
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"2eF 3h\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"fdghj\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"AbCd3\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart not the present Mowbray\n",
      "But the clouded the regree and men\n",
      "They shall chief thoughs so soul t\n",
      "\n",
      "\n",
      "\n",
      "What is here?\n",
      "\n",
      "CATESBY:\n",
      "Why, blush more action, loyal know my morning\n",
      "time. Let me not, holy hand in the ch\n",
      "\n",
      "\n",
      "\n",
      "Shall I give the bright his blood,\n",
      "Than be up the heavens, the counsel of the sile\n",
      "for the marters combard hath \n",
      "\n",
      "\n",
      "\n",
      "X087hNYB BHN BYFVuhsdbsoe: full\n",
      "The spirit death, for the faker!\n",
      "\n",
      "LUCIO:\n",
      "Mistressly soul will the cover for that put\n",
      "Finlma\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"The\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"What is\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"Shall I give\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"X087hNYB BHN BYFVuhsdbs\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
