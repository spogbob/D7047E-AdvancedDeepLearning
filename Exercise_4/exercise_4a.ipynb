{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Tensorboard for visualizing\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Tiny Shakespeare"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model from https://github.com/spro/char-rnn.pytorch/blob/master/model.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward2(self, input, hidden):\n",
    "        encoded = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### helpers.py from https://github.com/spro/char-rnn.pytorch/blob/master/helpers.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "# Reading and un-unicode-encoding data\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "# Turning a string into a tensor\n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor\n",
    "\n",
    "# Readable time elapsed\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate.py from https://github.com/spro/char-rnn.pytorch/blob/master/generate.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [46]\u001B[0m, in \u001B[0;36m<cell line: 56>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     54\u001B[0m decoder \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(filename)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m filename\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprime_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [46]\u001B[0m, in \u001B[0;36mgenerate\u001B[1;34m(decoder, prime_str, predict_len, temperature, cuda)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Use priming string to \"build up\" hidden state\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(prime_str) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 19\u001B[0m     _, hidden \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprime_input\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43mp\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m inp \u001B[38;5;241m=\u001B[39m prime_input[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(predict_len):\n",
      "File \u001B[1;32m~\\PycharmProjects\\D7047E-AdvancedDeepLearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [44]\u001B[0m, in \u001B[0;36mCharRNN.forward\u001B[1;34m(self, input, hidden)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, hidden):\n\u001B[0;32m     18\u001B[0m     batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m     encoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(encoded\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), hidden)\n\u001B[0;32m     21\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(output\u001B[38;5;241m.\u001B[39mview(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\D7047E-AdvancedDeepLearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\D7047E-AdvancedDeepLearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\D7047E-AdvancedDeepLearning\\venv\\lib\\site-packages\\torch\\nn\\functional.py:2183\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2177\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2178\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2179\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2180\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2181\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2182\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=True):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
    "\n",
    "    if cuda:\n",
    "        hidden = hidden.cuda()\n",
    "        prime_input = prime_input.cuda()\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "\n",
    "    inp = prime_input[:,-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "\n",
    "    return predicted\n",
    "\n",
    "# Run as standalone script\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "# Parse command line arguments\n",
    "#     argparser = argparse.ArgumentParser()\n",
    "#     argparser.add_argument('filename', type=str)\n",
    "#     argparser.add_argument('-p', '--prime_str', type=str, default='A')\n",
    "#     argparser.add_argument('-l', '--predict_len', type=int, default=100)\n",
    "#     argparser.add_argument('-t', '--temperature', type=float, default=0.8)\n",
    "#     argparser.add_argument('--cuda', action='store_true')\n",
    "#     args = argparser.parse_args()\n",
    "\n",
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"The\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train.py from https://github.com/spro/char-rnn.pytorch/blob/master/train.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training for 2000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [12:43<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12m 43s (2000 100%) 1.3494]\n",
      "What was gown some fan, and well:\n",
      "Therefore's my boy; they for then stay be she!\n",
      "\n",
      "LUCIO:\n",
      "Good master,  \n",
      "\n",
      "Saving...\n",
      "Saved as tinyShakespear.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Parse command line arguments\n",
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument('filename', type=str, default='tinyShakespear.txt')\n",
    "# argparser.add_argument('--model', type=str, default=\"gru\")\n",
    "# argparser.add_argument('--n_epochs', type=int, default=2000)\n",
    "# argparser.add_argument('--print_every', type=int, default=100)\n",
    "# argparser.add_argument('--hidden_size', type=int, default=100)\n",
    "# argparser.add_argument('--n_layers', type=int, default=2)\n",
    "# argparser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "# argparser.add_argument('--chunk_len', type=int, default=200)\n",
    "# argparser.add_argument('--batch_size', type=int, default=100)\n",
    "# argparser.add_argument('--shuffle', action='store_true')\n",
    "# argparser.add_argument('--cuda', action='store_true')\n",
    "# args = argparser.parse_args()\n",
    "\n",
    "arg_filename = \"tinyShakespear.txt\"\n",
    "arg_model = \"gru\"\n",
    "arg_n_epochs = 2000\n",
    "arg_print_every = 2000\n",
    "arg_hidden_size = 100\n",
    "arg_n_layers = 2\n",
    "arg_learning_rate = 0.01\n",
    "arg_chunk_len = 200\n",
    "arg_batch_size = 100\n",
    "arg_shuffle = True\n",
    "arg_cuda = True\n",
    "\n",
    "\n",
    "if arg_cuda:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "file, file_len = read_file(arg_filename)\n",
    "\n",
    "def random_training_set(chunk_len, batch_size):\n",
    "    inp = torch.LongTensor(batch_size, chunk_len)\n",
    "    target = torch.LongTensor(batch_size, chunk_len)\n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, file_len - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        inp[bi] = char_tensor(chunk[:-1])\n",
    "        target[bi] = char_tensor(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    if arg_cuda:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    return inp, target\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden(arg_batch_size)\n",
    "    if arg_cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(arg_chunk_len):\n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss += criterion(output.view(arg_batch_size, -1), target[:,c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/ arg_chunk_len\n",
    "\n",
    "def save():\n",
    "    save_filename = os.path.splitext(os.path.basename(arg_filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "# Initialize models and start training\n",
    "\n",
    "decoder = CharRNN(\n",
    "    n_characters,\n",
    "    arg_hidden_size,\n",
    "    n_characters,\n",
    "    model = arg_model,\n",
    "    n_layers = arg_n_layers\n",
    ")\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=arg_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if arg_cuda:\n",
    "    decoder.cuda()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "try:\n",
    "    print(\"Training for %d epochs...\" % arg_n_epochs)\n",
    "    for epoch in tqdm(range(1, arg_n_epochs + 1)):\n",
    "        loss = train(*random_training_set(arg_chunk_len, arg_batch_size))\n",
    "        writer.add_scalar(\"Perplexity\", np.exp(loss), epoch)\n",
    "        loss_avg += loss\n",
    "\n",
    "        if epoch % arg_print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / arg_n_epochs * 100, loss))\n",
    "            print(generate(decoder, 'Wh', 100, cuda=arg_cuda), '\\n')\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saving before quit...\")\n",
    "    save()\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Picture missing](img.png \"Validation accuracy each epoch\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"2eF 3h\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"fdghj\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"AbCd3\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"The\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"What is\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"Shall I give\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "start = \"X087hNYB BHN BYFVuhsdbs\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n",
    "print(\"\\n\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}