{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Tensorboard for visualizing\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Tiny Shakespeare"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model from https://github.com/spro/char-rnn.pytorch/blob/master/model.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward2(self, input, hidden):\n",
    "        encoded = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### helpers.py from https://github.com/spro/char-rnn.pytorch/blob/master/helpers.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "# Reading and un-unicode-encoding data\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "# Turning a string into a tensor\n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor\n",
    "\n",
    "# Readable time elapsed\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate.py from https://github.com/spro/char-rnn.pytorch/blob/master/generate.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which is, story \n",
      " ruin. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " wouldst and Which but \n",
      " congealed \n",
      " go \n",
      " \n",
      " \n",
      " \n",
      " by As Lord quickly that not wrong. \n",
      " Let needs or \n",
      " for the so \n",
      " \n",
      " is of a I A of the \n",
      " \n",
      " \n",
      " \n",
      " the \n",
      " the I III: what \n",
      " a \n",
      " the entreat I \n",
      " \n",
      " \n",
      " \n",
      " A woman and \n",
      " \n",
      " boy? \n",
      " \n",
      " in have \n",
      " \n",
      " \n",
      " we did \n",
      " \n",
      " of yet so Would \n",
      " no. or is my \n",
      " in in come \n",
      " III: \n",
      " Cupid's brother's \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def generate(decoder, prime_str=\"A\", predict_len=100, temperature=0.8, cuda=False):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_words = make_trainset(prime_str, len(prime_str))\n",
    "    start_seq = get_index_vocab(vocab,prime_words)\n",
    "    prime_input = Variable(torch.as_tensor(start_seq).unsqueeze(0))\n",
    "\n",
    "    if cuda:\n",
    "        hidden = hidden.cuda()\n",
    "        prime_input = prime_input.cuda()\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_words) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "\n",
    "    inp = prime_input[:,-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_word = vocab[top_i]\n",
    "        predicted += predicted_word + \" \"\n",
    "        inp = Variable(torch.as_tensor(top_i).unsqueeze(0))\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "\n",
    "    return predicted\n",
    "\n",
    "# Run as standalone script\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "# Parse command line arguments\n",
    "#     argparser = argparse.ArgumentParser()\n",
    "#     argparser.add_argument('filename', type=str)\n",
    "#     argparser.add_argument('-p', '--prime_str', type=str, default='A')\n",
    "#     argparser.add_argument('-l', '--predict_len', type=int, default=100)\n",
    "#     argparser.add_argument('-t', '--temperature', type=float, default=0.8)\n",
    "#     argparser.add_argument('--cuda', action='store_true')\n",
    "#     args = argparser.parse_args()\n",
    "\n",
    "filename = \"Task4.pt\"\n",
    "start = \"which is, \"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train.py from https://github.com/spro/char-rnn.pytorch/blob/master/train.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1999/2000 [1:23:00<00:02,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83m 0s (2000 100%) 2.9205]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [87]\u001B[0m, in \u001B[0;36m<cell line: 157>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m arg_print_every \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;132;01m%%\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;132;01m%.4f\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (time_since(start), epoch, epoch \u001B[38;5;241m/\u001B[39m arg_n_epochs \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m, loss))\n\u001B[1;32m--> 166\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWh\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_cuda\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    169\u001B[0m save()\n",
      "Input \u001B[1;32mIn [79]\u001B[0m, in \u001B[0;36mgenerate\u001B[1;34m(decoder, prime_str, predict_len, temperature, cuda)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(prime_words) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     21\u001B[0m     _, hidden \u001B[38;5;241m=\u001B[39m decoder(prime_input[:,p], hidden)\n\u001B[1;32m---> 23\u001B[0m inp \u001B[38;5;241m=\u001B[39m \u001B[43mprime_input\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(predict_len):\n\u001B[0;32m     26\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m decoder(inp, hidden)\n",
      "\u001B[1;31mIndexError\u001B[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Parse command line arguments\n",
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument('filename', type=str, default='tinyShakespear.txt')\n",
    "# argparser.add_argument('--model', type=str, default=\"gru\")\n",
    "# argparser.add_argument('--n_epochs', type=int, default=2000)\n",
    "# argparser.add_argument('--print_every', type=int, default=100)\n",
    "# argparser.add_argument('--hidden_size', type=int, default=100)\n",
    "# argparser.add_argument('--n_layers', type=int, default=2)\n",
    "# argparser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "# argparser.add_argument('--chunk_len', type=int, default=200)\n",
    "# argparser.add_argument('--batch_size', type=int, default=100)\n",
    "# argparser.add_argument('--shuffle', action='store_true')\n",
    "# argparser.add_argument('--cuda', action='store_true')\n",
    "# args = argparser.parse_args()\n",
    "\n",
    "arg_filename = \"tinyShakespear.txt\"\n",
    "arg_model = \"gru\"\n",
    "arg_n_epochs = 2000\n",
    "arg_print_every = 2000\n",
    "arg_hidden_size = 100\n",
    "arg_n_layers = 2\n",
    "arg_learning_rate = 0.01\n",
    "arg_chunk_len = 20\n",
    "arg_batch_size = 200\n",
    "arg_shuffle = True\n",
    "arg_cuda = False\n",
    "\n",
    "\n",
    "if arg_cuda:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "file, file_len = read_file(arg_filename)\n",
    "\n",
    "def make_trainset(input_seq, input_len):\n",
    "    my_vocab = []\n",
    "    my_word = \"\"\n",
    "    for i in range(input_len):\n",
    "        my_char = input_seq[i]\n",
    "        if my_char == ' ' or my_char == '\\0':\n",
    "            if len(my_word) > 0:\n",
    "                my_vocab.append(my_word)\n",
    "            my_word = \"\"\n",
    "        elif my_char == '\\n':\n",
    "            if len(my_word) > 0:\n",
    "                my_vocab.append(my_word)\n",
    "            my_vocab.append('\\n')\n",
    "            my_word = \"\"\n",
    "        else:\n",
    "            my_word = my_word + my_char\n",
    "    if len(my_word) > 0:\n",
    "        my_vocab.append(my_word)\n",
    "    return my_vocab\n",
    "\n",
    "def make_vocab(input_seq, input_len):\n",
    "    my_vocab = ['\\n']\n",
    "    my_word = \"\"\n",
    "    for i in range(input_len):\n",
    "        my_char = input_seq[i]\n",
    "        if my_char == ' ' or my_char == '\\n' or my_char == '\\0':\n",
    "            if len(my_word) > 0:\n",
    "                if my_word not in my_vocab:\n",
    "                    my_vocab.append(my_word)\n",
    "            my_word = \"\"\n",
    "        else:\n",
    "            my_word = my_word + my_char\n",
    "\n",
    "    if len(my_word) > 0:\n",
    "        if my_word not in my_vocab:\n",
    "            my_vocab.append(my_word)\n",
    "\n",
    "    return my_vocab\n",
    "\n",
    "def get_index_vocab(my_vocab, chunk):\n",
    "    index_list = []\n",
    "    for word in chunk:\n",
    "        try:\n",
    "            index_list.append(my_vocab.index(word))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return index_list\n",
    "\n",
    "vocab = make_vocab(file, file_len)\n",
    "trainset = make_trainset(file,file_len)\n",
    "\n",
    "def random_training_set(chunk_len, batch_size):\n",
    "    inp = torch.LongTensor(batch_size, chunk_len)\n",
    "    target = torch.LongTensor(batch_size, chunk_len)\n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, len(trainset) - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = trainset[start_index:end_index]\n",
    "        chunk = get_index_vocab(vocab, chunk)\n",
    "        inp[bi] = torch.as_tensor(chunk[:-1])\n",
    "        target[bi] = torch.as_tensor(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    if arg_cuda:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    return inp, target\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden(arg_batch_size)\n",
    "    if arg_cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(arg_chunk_len):\n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss += criterion(output.view(arg_batch_size, -1), target[:,c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/ arg_chunk_len\n",
    "\n",
    "def save():\n",
    "    save_filename = 'Task4.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "# Initialize models and start training\n",
    "\n",
    "decoder = CharRNN(\n",
    "    len(vocab),\n",
    "    arg_hidden_size,\n",
    "    len(vocab),\n",
    "    model = arg_model,\n",
    "    n_layers = arg_n_layers\n",
    ")\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=arg_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if arg_cuda:\n",
    "    decoder.cuda()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "try:\n",
    "    print(\"Training for %d epochs...\" % arg_n_epochs)\n",
    "    for epoch in tqdm(range(1, arg_n_epochs + 1)):\n",
    "        loss = train(*random_training_set(arg_chunk_len, arg_batch_size))\n",
    "        writer.add_scalar(\"Perplexity\", np.exp(loss), epoch)\n",
    "        loss_avg += loss\n",
    "\n",
    "        if epoch % arg_print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / arg_n_epochs * 100, loss))\n",
    "            print(generate(decoder, 'Wh', 100, cuda=arg_cuda), '\\n')\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saving before quit...\")\n",
    "    save()\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Picture missing](img_1.png \"Validation accuracy each epoch\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vineyard by the to have you \n",
      " it \n",
      " rehearse, not \n",
      " \n",
      " A the \n",
      " \n",
      " none \n",
      " is you \n",
      " to \n",
      " of then some what even obstacles: three peace: \n",
      " with \n",
      " \n",
      " \n",
      " to if \n",
      " \n",
      " \n",
      " to \n",
      " \n",
      " \n",
      " time \n",
      " is \n",
      " that the he of \n",
      " their to God I \n",
      " \n",
      " \n",
      " be York \n",
      " give \n",
      " \n",
      " of his \n",
      " THOMAS king. of \n",
      " \n",
      " \n",
      " restored your \n",
      " \n",
      " \n",
      " LEONTES: \n",
      " thy since \n",
      " \n",
      " against \n",
      " \n",
      " up \n",
      " benefit \n",
      " of \n",
      " my on \n",
      " \n"
     ]
    }
   ],
   "source": [
    "filename = \"Task4.pt\"\n",
    "start = \"The \"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [83]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m start \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprime_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [79]\u001B[0m, in \u001B[0;36mgenerate\u001B[1;34m(decoder, prime_str, predict_len, temperature, cuda)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(prime_words) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     21\u001B[0m     _, hidden \u001B[38;5;241m=\u001B[39m decoder(prime_input[:,p], hidden)\n\u001B[1;32m---> 23\u001B[0m inp \u001B[38;5;241m=\u001B[39m \u001B[43mprime_input\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(predict_len):\n\u001B[0;32m     26\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m decoder(inp, hidden)\n",
      "\u001B[1;31mIndexError\u001B[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "start = \",\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which is,Of Signior to man off \n",
      " \n",
      " \n",
      " \n",
      " this GLOUCESTER: Forgive the \n",
      " the know the he this \n",
      " lord; So \n",
      " set \n",
      " \n",
      " \n",
      " to good \n",
      " man's wants, make not and \n",
      " handsome to draw \n",
      " and for \n",
      " \n",
      " O, \n",
      " \n",
      " \n",
      " LUCIO: But \n",
      " mind \n",
      " \n",
      " \n",
      " the \n",
      " to \n",
      " till \n",
      " \n",
      " is dear imagine \n",
      " so they \n",
      " \n",
      " \n",
      " he \n",
      " \n",
      " this the And have \n",
      " the friendly Iniquity? \n",
      " \n",
      " \n",
      " \n",
      " me \n",
      " \n",
      " their \n",
      " will \n",
      " fault \n",
      " \n",
      " it shall thick-ribbed \n",
      " \n"
     ]
    }
   ],
   "source": [
    "start = \"which is,\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [85]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m start \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblah blah blah\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprime_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [79]\u001B[0m, in \u001B[0;36mgenerate\u001B[1;34m(decoder, prime_str, predict_len, temperature, cuda)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Use priming string to \"build up\" hidden state\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(prime_words) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 21\u001B[0m     _, hidden \u001B[38;5;241m=\u001B[39m decoder(\u001B[43mprime_input\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43mp\u001B[49m\u001B[43m]\u001B[49m, hidden)\n\u001B[0;32m     23\u001B[0m inp \u001B[38;5;241m=\u001B[39m prime_input[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(predict_len):\n",
      "\u001B[1;31mIndexError\u001B[0m: index 0 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "start = \"blah blah blah\"\n",
    "\n",
    "print(generate(decoder, prime_str=start))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}