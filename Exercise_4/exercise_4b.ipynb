{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Tensorboard for visualizing\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Tiny Shakespeare"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model from https://github.com/spro/char-rnn.pytorch/blob/master/model.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward2(self, input, hidden):\n",
    "        encoded = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### helpers.py from https://github.com/spro/char-rnn.pytorch/blob/master/helpers.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "# Reading and un-unicode-encoding data\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "# Turning a string into a tensor\n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor\n",
    "\n",
    "# Readable time elapsed\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate.py from https://github.com/spro/char-rnn.pytorch/blob/master/generate.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hear me here all him.\n",
      "\n",
      "CORIO:\n",
      "How fares been, sir, that he at this pition,\n",
      "No fortune, as the times\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=True):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    start_seq = get_index_vocab(vocab,prime_str)\n",
    "    prime_input = Variable(torch.as_tensor(start_seq).unsqueeze(0))\n",
    "\n",
    "    if cuda:\n",
    "        hidden = hidden.cuda()\n",
    "        prime_input = prime_input.cuda()\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "\n",
    "    inp = prime_input[:,-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_word = vocab[top_i]\n",
    "        predicted += predicted_word + \" \"\n",
    "        inp = Variable(torch.as_tensor(top_i).unsqueeze(0))\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "\n",
    "    return predicted\n",
    "\n",
    "# Run as standalone script\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "# Parse command line arguments\n",
    "#     argparser = argparse.ArgumentParser()\n",
    "#     argparser.add_argument('filename', type=str)\n",
    "#     argparser.add_argument('-p', '--prime_str', type=str, default='A')\n",
    "#     argparser.add_argument('-l', '--predict_len', type=int, default=100)\n",
    "#     argparser.add_argument('-t', '--temperature', type=float, default=0.8)\n",
    "#     argparser.add_argument('--cuda', action='store_true')\n",
    "#     args = argparser.parse_args()\n",
    "\n",
    "filename = \"tinyShakespear.pt\"\n",
    "start = \"The\"\n",
    "\n",
    "decoder = torch.load(filename)\n",
    "del filename\n",
    "print(generate(decoder, prime_str=start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train.py from https://github.com/spro/char-rnn.pytorch/blob/master/train.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Training for 200 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1142, 199, 35, 27979, 102, 48707, 53258, 90, 27964, 102, 61, 126374, 130, 126376, 1142, 31285, 102, 61, 47614, 29093, 126383, 3796, 1483, 39, 4066, 20, 57, 85, 126391, 11777, 31, 256, 126395, 55490, 1836, 102]\n",
      "[199, 35, 27979, 102, 48707, 53258, 90, 27964, 102, 61, 126374, 130, 126376, 1142, 31285, 102, 61, 47614, 29093, 126383, 3796, 1483, 39, 4066, 20, 57, 85, 126391, 11777, 31, 256, 126395, 55490, 1836, 102, 61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [39]\u001B[0m, in \u001B[0;36m<cell line: 138>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining for \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m epochs...\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m arg_n_epochs)\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, arg_n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[1;32m--> 141\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrandom_training_set\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_chunk_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg_batch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m     writer\u001B[38;5;241m.\u001B[39madd_scalar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPerplexity\u001B[39m\u001B[38;5;124m\"\u001B[39m, np\u001B[38;5;241m.\u001B[39mexp(loss), epoch)\n\u001B[0;32m    143\u001B[0m     loss_avg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n",
      "Input \u001B[1;32mIn [39]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(inp, target)\u001B[0m\n\u001B[0;32m    101\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(inp)):\n\u001B[1;32m--> 104\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[43m[\u001B[49m\u001B[43mc\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    105\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m criterion(output\u001B[38;5;241m.\u001B[39mview(arg_batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), target[:,c])\n\u001B[0;32m    107\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\PycharmProjects\\D7047E-AdvancedDeepLearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mCharRNN.forward\u001B[1;34m(self, input, hidden)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, hidden):\n\u001B[1;32m---> 18\u001B[0m     batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     encoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m     20\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(encoded\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), hidden)\n",
      "\u001B[1;31mIndexError\u001B[0m: dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Parse command line arguments\n",
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.add_argument('filename', type=str, default='tinyShakespear.txt')\n",
    "# argparser.add_argument('--model', type=str, default=\"gru\")\n",
    "# argparser.add_argument('--n_epochs', type=int, default=2000)\n",
    "# argparser.add_argument('--print_every', type=int, default=100)\n",
    "# argparser.add_argument('--hidden_size', type=int, default=100)\n",
    "# argparser.add_argument('--n_layers', type=int, default=2)\n",
    "# argparser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "# argparser.add_argument('--chunk_len', type=int, default=200)\n",
    "# argparser.add_argument('--batch_size', type=int, default=100)\n",
    "# argparser.add_argument('--shuffle', action='store_true')\n",
    "# argparser.add_argument('--cuda', action='store_true')\n",
    "# args = argparser.parse_args()\n",
    "\n",
    "arg_filename = \"tinyShakespear.txt\"\n",
    "arg_model = \"gru\"\n",
    "arg_n_epochs = 200\n",
    "arg_print_every = 2000\n",
    "arg_hidden_size = 100\n",
    "arg_n_layers = 2\n",
    "arg_learning_rate = 0.01\n",
    "arg_chunk_len = 200\n",
    "arg_batch_size = 1\n",
    "arg_shuffle = True\n",
    "arg_cuda = True\n",
    "\n",
    "\n",
    "if arg_cuda:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "file, file_len = read_file(arg_filename)\n",
    "\n",
    "def make_vocab(input_seq, input_len):\n",
    "    my_vocab = []\n",
    "    my_word = \"\"\n",
    "    for i in range(input_len):\n",
    "        my_char = input_seq[i]\n",
    "        if my_char == ' ' or my_char == '\\n' or my_char == '\\0':\n",
    "            if len(my_word) > 0:\n",
    "                my_vocab.append(my_word)\n",
    "            my_word = \"\"\n",
    "        else:\n",
    "            my_word = my_word + my_char\n",
    "    return my_vocab\n",
    "\n",
    "def get_index_vocab(vocab, chunk):\n",
    "    index_list = []\n",
    "    my_word = \"\"\n",
    "    for i in range(len(chunk)):\n",
    "        my_char = chunk[i]\n",
    "        if my_char == ' ' or my_char == '\\n' or my_char == '\\0':\n",
    "            if len(my_word) > 0:\n",
    "                try:\n",
    "                    index_list.append(vocab.index(my_word))\n",
    "                except:\n",
    "                    continue\n",
    "            my_word = \"\"\n",
    "        else:\n",
    "            my_word = my_word + my_char\n",
    "    return index_list\n",
    "\n",
    "vocab = make_vocab(file, file_len)\n",
    "\n",
    "def random_training_set(chunk_len, batch_size):\n",
    "    # inp = torch.LongTensor(batch_size, chunk_len)\n",
    "    # target = torch.LongTensor(batch_size, chunk_len)\n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, file_len - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        chunk = get_index_vocab(vocab, chunk)\n",
    "        print(chunk[0:-2])\n",
    "        print(chunk[1:-1])\n",
    "        inp = torch.as_tensor(chunk[:-1])\n",
    "        target = torch.as_tensor(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    if arg_cuda:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    return inp, target\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden(arg_batch_size)\n",
    "    if arg_cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(len(inp)):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output.view(arg_batch_size, -1), target[:,c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/ arg_chunk_len\n",
    "\n",
    "def save():\n",
    "    save_filename = os.path.splitext(os.path.basename(arg_filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "# Initialize models and start training\n",
    "\n",
    "decoder = CharRNN(\n",
    "    n_characters,\n",
    "    arg_hidden_size,\n",
    "    n_characters,\n",
    "    model = arg_model,\n",
    "    n_layers = arg_n_layers\n",
    ")\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=arg_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if arg_cuda:\n",
    "    decoder.cuda()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "try:\n",
    "    print(\"Training for %d epochs...\" % arg_n_epochs)\n",
    "    for epoch in tqdm(range(1, arg_n_epochs + 1)):\n",
    "        loss = train(*random_training_set(arg_chunk_len, arg_batch_size))\n",
    "        writer.add_scalar(\"Perplexity\", np.exp(loss), epoch)\n",
    "        loss_avg += loss\n",
    "\n",
    "        if epoch % arg_print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / arg_n_epochs * 100, loss))\n",
    "            print(generate(decoder, 'Wh', 100, cuda=arg_cuda), '\\n')\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saving before quit...\")\n",
    "    save()\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speak.\n"
     ]
    }
   ],
   "source": [
    "print(vocab[12])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2eF 3hom at your privilued, nor\n",
      "Some company, our husband of not to pursue.\n",
      "\n",
      "RICHMOND:\n",
      "No loddle of a fire\n",
      "\n",
      "\n",
      "\n",
      "fdghject you to twell'd, sir.\n",
      "\n",
      "Nurse:\n",
      "The shoppited conference, uncle, stay, but no,\n",
      "Which art thou bed, \n",
      "\n",
      "\n",
      "\n",
      "AbCd3 GOZUMWARD:\n",
      "So shall I prepore, and alive me up.\n",
      "3 KING HENRY VI\n",
      "\n",
      "PROSPERO:\n",
      "Before; thyself at the p\n"
     ]
    }
   ],
   "source": [
    " def simple_tokenizer(text):\n",
    "    '''\n",
    "    Takes a text and returns a list of the individual words\n",
    "    Args:\n",
    "        text (str): The text to tokenize\n",
    "    Returns ([str]): A list of individual words\n",
    "    '''\n",
    "    ## WRITE A TOKENIZER (Hint: use the 'split' function)\n",
    "    return text.split()\n",
    "\n",
    "class SimpleVocabulary():\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        '''\n",
    "        Takes a corpus, tokenizes it, and sets the internal variables needed\n",
    "        to return the index of any given word (and what word has a given index)\n",
    "        Args:\n",
    "            corpus ([str]): The corpus as a list of non-tokenized texts\n",
    "        '''\n",
    "        ## WRITE A FUNCTION TO INITIALIZE THE VOCABULARY\n",
    "        # HINT: Create a dictionary that contains the index for each unique word and antother to contain the word for a given index\n",
    "        # HINT: Each new, previously unseen token from the tokenized corpus, should be entered into the dictionaries\n",
    "\n",
    "        current_index = 0\n",
    "        self.token2index = {}\n",
    "        self.index2token = []\n",
    "        for text in corpus:\n",
    "            for word in simple_tokenizer(text):\n",
    "                if word not in self.token2index:\n",
    "                    self.index2token.append(word)\n",
    "                    self.token2index.update({word : current_index})\n",
    "                    current_index += 1\n",
    "\n",
    "\n",
    "    def get_index(self, token):\n",
    "        '''\n",
    "        Takes a token and returns the index of that token\n",
    "        Args:\n",
    "            token (str): A token\n",
    "        Returns (int): The index of the given token\n",
    "        '''\n",
    "        ## WRITE A FUNCTION THAT RETURNS THE INDEX OF A GIVEN TOKEN\n",
    "        return self.token2index.get(token)\n",
    "\n",
    "\n",
    "    def get_token(self, index):\n",
    "        '''\n",
    "        Takes an index and returns the token that index represents\n",
    "        Args:\n",
    "            index (int): An index\n",
    "        Returns (str): The token that the given index represents\n",
    "        '''\n",
    "        ## WRITE A FUNCTION THAT RETURNS THE TOKEN OF A GIVEN INDEX\n",
    "        return self.index2token[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns (int): The number of words in the vocabulary\n",
    "        '''\n",
    "        ## WRITE A FUNCTION THAT RETURNS THE LENGTH OF THE VOCABULARY\n",
    "        return len(self.token2index)\n",
    "\n",
    "def index_sequence(sequence, vocabulary):\n",
    "    '''\n",
    "    Takes a tokenized text and a vocabulary and returns an indexed sequence\n",
    "    Args:\n",
    "        sequence ([str]): A list of individual tokens\n",
    "    Returns ([int]): A list of indexes\n",
    "    '''\n",
    "\n",
    "    return [vocabulary.get_index(token) for token in sequence]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart not the present Mowbray\n",
      "But the clouded the regree and men\n",
      "They shall chief thoughs so soul t\n",
      "\n",
      "\n",
      "\n",
      "What is here?\n",
      "\n",
      "CATESBY:\n",
      "Why, blush more action, loyal know my morning\n",
      "time. Let me not, holy hand in the ch\n",
      "\n",
      "\n",
      "\n",
      "Shall I give the bright his blood,\n",
      "Than be up the heavens, the counsel of the sile\n",
      "for the marters combard hath \n",
      "\n",
      "\n",
      "\n",
      "X087hNYB BHN BYFVuhsdbsoe: full\n",
      "The spirit death, for the faker!\n",
      "\n",
      "LUCIO:\n",
      "Mistressly soul will the cover for that put\n",
      "Finlma\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}